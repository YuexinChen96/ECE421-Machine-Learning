{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "dropout = False\n",
    "reg_on = False\n",
    "reg = 0.5\n",
    "\n",
    "\n",
    "# Load the data\n",
    "def loadData():\n",
    "    with np.load(\"notMNIST.npz\") as data:\n",
    "        Data, Target = data[\"images\"], data[\"labels\"]\n",
    "        np.random.seed(521)\n",
    "        randIndx = np.arange(len(Data))\n",
    "        np.random.shuffle(randIndx)\n",
    "        Data = Data[randIndx] / 255.0\n",
    "        Target = Target[randIndx]\n",
    "        trainData, trainTarget = Data[:10000], Target[:10000]\n",
    "        validData, validTarget = Data[10000:16000], Target[10000:16000]\n",
    "        testData, testTarget = Data[16000:], Target[16000:]\n",
    "    return trainData, validData, testData, trainTarget, validTarget, testTarget\n",
    "def convertOneHot(trainTarget, validTarget, testTarget):\n",
    "    newtrain = np.zeros((trainTarget.shape[0], 10))\n",
    "    newvalid = np.zeros((validTarget.shape[0], 10))\n",
    "    newtest = np.zeros((testTarget.shape[0], 10))\n",
    "\n",
    "    for item in range(0, trainTarget.shape[0]):\n",
    "        newtrain[item][trainTarget[item]] = 1\n",
    "    for item in range(0, validTarget.shape[0]):\n",
    "        newvalid[item][validTarget[item]] = 1\n",
    "    for item in range(0, testTarget.shape[0]):\n",
    "        newtest[item][testTarget[item]] = 1\n",
    "    return newtrain, newvalid, newtest\n",
    "def shuffle(trainData, trainTarget):\n",
    "    np.random.seed(421)\n",
    "    randIndx = np.arange(len(trainData))\n",
    "    target = trainTarget\n",
    "    np.random.shuffle(randIndx)\n",
    "    data, target = trainData[randIndx], target[randIndx]\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_init(features, labels, d_s=28*28, initializer=tf.contrib.layers.xavier_initializer()):\n",
    "    #Initialize weight and bias tensors\n",
    "    tf.set_random_seed(421)\n",
    "    mode = tf.placeholder(tf.bool)\n",
    "    k32 = tf.placeholder(tf.float32)\n",
    "    #1. input layer\n",
    "    X = tf.reshape(features,shape = [-1,28,28,1])\n",
    "    #Regularizer\n",
    "    #reg = tf.contrib.layers.l2_regularizer(scale=)\n",
    "    #2. 3*3 convolution\n",
    "    W1 = tf.get_variable(\"W1\",[3,3,1,32],dtype='float64',initializer=initializer)\n",
    "    b1 = tf.get_variable(\"b1\",[32],dtype='float64',initializer=initializer)\n",
    "    conv = tf.nn.conv2d(X,W1,strides=[1,1,1,1],padding='SAME')\n",
    "    #3. Relu activation\n",
    "    conv_R = tf.nn.relu(conv+b1,name='conv_R')\n",
    "    #4. Batch normalization\n",
    "    mean, var = tf.nn.moments(conv_R,axes=[0,1,2])\n",
    "    bn = tf.nn.batch_normalization(x=conv_R,mean=mean,variance=var,offset=None,scale=None,variance_epsilon=0.001)\n",
    "    #5. 2*2 max pooling\n",
    "    pool = tf.nn.max_pool(bn,ksize=[1,2,2,1],strides=[1,1,1,1],padding='SAME')\n",
    "    #6. Flatten\n",
    "    pool = tf.reshape(pool,[-1,6277])\n",
    "    #7. Fully connected layer\n",
    "    W2 = tf.get_variable(\"W2\",[6272,1024],dtype='float64',initializer=initializer)\n",
    "    b2 = tf.get_variable(\"b2\",[1024],dtype='float64',initializer=initializer)\n",
    "    #8. Relu activation / dropout\n",
    "    fully_c1 = tf.matmul(pool,W2)+b2\n",
    "    if dropout:\n",
    "        drop = tf.nn.dropout(fully_c1,k32)\n",
    "        fully_c1 = tf.nn.relu(drop)\n",
    "    else:\n",
    "        fully_c1 = tf.nn.relu(fully_c1)\n",
    "    #9. Fully connected layer\n",
    "    W3 = tf.get_variable(\"W3\",[1024,10],dtype='float64',initializer=initializer)\n",
    "    b3 = tf.get_variable(\"b3\",[10],dtype='float64',initializer=initializer)\n",
    "    #10. Softmax\n",
    "    fully_c2 = tf.matmul(fully_c1,W3)+b3\n",
    "    #11. Cross Entropy Loss\n",
    "    en = tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels,logits=fully_c2)\n",
    "    loss = tf.reduce_mean(en)\n",
    "    #Regulization\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolutional_layers(features, labels):\n",
    "    # Input Layer\n",
    "    input = tf.reshape(features, shape=[-1, 28, 28, 1])\n",
    "\n",
    "    # 3x3 convolution, 1 input, 32 outputs\n",
    "    W1 = tf.get_variable(\"W1\", [3, 3, 1, 32], dtype='float64',initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b1 = tf.get_variable('b1', [32], dtype='float64', initializer=tf.contrib.layers.xavier_initializer())\n",
    "    conv = tf.nn.conv2d(input, W1, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    conv1 = tf.nn.relu(conv + b1, name='conv1')\n",
    "\n",
    "    # Batch Normalization layer\n",
    "    mean, variance = tf.nn.moments(conv1, axes=[0, 1, 2])\n",
    "    bn = tf.nn.batch_normalization(x=conv1, mean=mean, variance=variance, offset=None, scale=None, variance_epsilon=0.001)\n",
    "\n",
    "    # 2Ã—2 max pooling layer\n",
    "    pool = tf.nn.max_pool(bn, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "    # Flatten Layer\n",
    "    pool = tf.reshape(pool, [-1, 6272])\n",
    "\n",
    "    # Fully connected layer relu\n",
    "    W2 = tf.get_variable('W2', [6272, 1024], dtype='float64', initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b2 = tf.get_variable('b2', [1024], dtype='float64', initializer=tf.contrib.layers.xavier_initializer())\n",
    "    fc1 = tf.nn.relu(tf.matmul(pool, W2) + b2)\n",
    "\n",
    "    # Fully connected layer with softmax\n",
    "    W3 = tf.get_variable('W3', [1024, 10], dtype='float64', initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b3 = tf.get_variable('b3', [10], dtype='float64', initializer=tf.contrib.layers.xavier_initializer())\n",
    "    fc2 = tf.matmul(fc1, W3) + b3\n",
    "\n",
    "    # Loss with cross entropy\n",
    "    entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels, logits=fc2)\n",
    "    loss = tf.reduce_mean(entropy)\n",
    "\n",
    "    return loss, W3, b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model_Training(features, labels):\n",
    "\n",
    "    dim = 10\n",
    "    N = features.shape[0]\n",
    "    dim_x = features.shape[1]\n",
    "    dim_y = features.shape[2]\n",
    "    batch_size = 32\n",
    "    epoch = 50\n",
    "    runs = int(N / batch_size)\n",
    "\n",
    "\n",
    "    loss, W, b = convolutional_layers(features, labels)\n",
    "    print('hi')\n",
    "    x_data = features\n",
    "    y_data = labels\n",
    "\n",
    "\n",
    "    # Define placeholders to feed mini_batches\n",
    "    X = tf.placeholder(tf.float32, shape=(batch_size, dim_x*dim_y), name=\"X\")\n",
    "    Y = tf.placeholder(tf.float32, shape=(batch_size, None), name=\"Y\")\n",
    "\n",
    "    opt = tf.train.AdamOptimizer(0.0001).minimize(loss)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    session = tf.Session()\n",
    "    session.run(init)\n",
    "\n",
    "    # Fit the line.\n",
    "    for s in range(epoch):\n",
    "        for p in range(runs):\n",
    "\n",
    "            x_batch = x_data[p * batch_size:(p + 1) * batch_size].reshape((batch_size, x_data.shape[1]*x_data.shape[2]))\n",
    "            y_batch = y_data[p * batch_size:(p + 1) * batch_size]\n",
    "            print(y_batch.shape)\n",
    "            _, l = session.run([opt, loss], feed_dict={X: x_batch, Y: y_batch})\n",
    "\n",
    "            if s % 1 == 0:\n",
    "                print(s, session.run(W))\n",
    "\n",
    "        x_data, y_data = shuffle(x_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n",
      "(32, 10)\n"
     ]
    }
   ],
   "source": [
    "trainData, validData, testData, trainTarget, validTarget, testTarget = loadData()\n",
    "trainTarget, validTarget, testTarget = convertOneHot(trainTarget, validTarget, testTarget)\n",
    "Model_Training(trainData, trainTarget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
