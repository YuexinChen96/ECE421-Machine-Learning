{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "dropout = False\n",
    "reg = 0\n",
    "\n",
    "# Load the data\n",
    "def loadData():\n",
    "    with np.load(\"notMNIST.npz\") as data:\n",
    "        Data, Target = data[\"images\"], data[\"labels\"]\n",
    "        np.random.seed(521)\n",
    "        randIndx = np.arange(len(Data))\n",
    "        np.random.shuffle(randIndx)\n",
    "        Data = Data[randIndx] / 255.0\n",
    "        Target = Target[randIndx]\n",
    "        trainData, trainTarget = Data[:10000], Target[:10000]\n",
    "        validData, validTarget = Data[10000:16000], Target[10000:16000]\n",
    "        testData, testTarget = Data[16000:], Target[16000:]\n",
    "    return trainData, validData, testData, trainTarget, validTarget, testTarget\n",
    "\n",
    "# Implementation of a neural network using only Numpy - trained using gradient descent with momentum\n",
    "def convertOneHot(trainTarget, validTarget, testTarget):\n",
    "    newtrain = np.zeros((trainTarget.shape[0], 10))\n",
    "    newvalid = np.zeros((validTarget.shape[0], 10))\n",
    "    newtest = np.zeros((testTarget.shape[0], 10))\n",
    "    for item in range(0, trainTarget.shape[0]):\n",
    "        newtrain[item][trainTarget[item]] = 1\n",
    "    for item in range(0, validTarget.shape[0]):\n",
    "        newvalid[item][validTarget[item]] = 1\n",
    "    for item in range(0, testTarget.shape[0]):\n",
    "        newtest[item][testTarget[item]] = 1\n",
    "    return newtrain, newvalid, newtest\n",
    "#data like ==> newtrain[0-9999][0-9]\n",
    "\n",
    "def shuffle(trainData, trainTarget):\n",
    "    np.random.seed(421)\n",
    "    randIndx = np.arange(len(trainData))\n",
    "    target = trainTarget\n",
    "    np.random.shuffle(randIndx)\n",
    "    data, target = trainData[randIndx], target[randIndx]\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolutional_layers(features, labels):\n",
    "    # Input Layer\n",
    "    input = tf.reshape(features, shape=[-1, 28, 28, 1])\n",
    "    # 3x3 convolution, 1 input, 32 outputs\n",
    "    W1 = tf.get_variable(\"W1\", [3, 3, 1, 32], dtype='float32',initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b1 = tf.get_variable('b1', [32], dtype='float32', initializer=tf.contrib.layers.xavier_initializer())\n",
    "    conv = tf.nn.conv2d(input, W1, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "    conv1 = tf.nn.relu(conv + b1, name='conv1')\n",
    "\n",
    "    # Batch Normalization layer\n",
    "    mean, variance = tf.nn.moments(conv1, axes=[0, 1, 2])\n",
    "    bn = tf.nn.batch_normalization(x=conv1, mean=mean, variance=variance, offset=None, scale=None, variance_epsilon=0.001)\n",
    "\n",
    "    # 2Ã—2 max pooling layer\n",
    "    pool = tf.nn.max_pool(bn, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "    # Flatten Layer\n",
    "    pool = tf.reshape(pool, [-1, 6272])\n",
    "\n",
    "    # Fully connected layer relu\n",
    "    W2 = tf.get_variable('W2', [6272, 1024], dtype='float32', initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b2 = tf.get_variable('b2', [1024], dtype='float32', initializer=tf.contrib.layers.xavier_initializer())\n",
    "    fc1 = tf.nn.relu(tf.matmul(pool, W2) + b2)\n",
    "\n",
    "    # Fully connected layer with softmax\n",
    "    W3 = tf.get_variable('W3', [1024, 10], dtype='float32', initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b3 = tf.get_variable('b3', [10], dtype='float32', initializer=tf.contrib.layers.xavier_initializer())\n",
    "    #sm = tf.get_variable('sm', [labels[0].shape, labels[1].shape], dtype='float64', initializer=tf.contrib.layers.xavier_initializer())\n",
    "    fc2 = tf.matmul(fc1, W3) + b3\n",
    "    sm = tf.nn.softmax(fc2)\n",
    "    acc, acc_op = tf.metrics.accuracy(labels=tf.argmax(sm, 1), predictions=tf.argmax(labels, 1))\n",
    "\n",
    "    # Loss with cross entropy\n",
    "    entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels, logits=fc2)\n",
    "    loss = tf.reduce_mean(entropy)\n",
    "\n",
    "    return loss, W3, b3, acc_op\n",
    "\n",
    "def Model_Training(features, labels):\n",
    "\n",
    "    dim = 10\n",
    "    N = features.shape[0]\n",
    "    dim_x = features.shape[1]\n",
    "    dim_y = features.shape[2]\n",
    "    batch_size = 32\n",
    "    epoch = 50\n",
    "    runs = int(N / batch_size)\n",
    "\n",
    "    # Define placeholders to feed mini_batches\n",
    "    X = tf.placeholder(tf.float32, shape=(batch_size, dim_x * dim_y), name=\"X\")\n",
    "    Y = tf.placeholder(tf.float32, shape=(batch_size, None), name=\"Y\")\n",
    "    lam = tf.placeholder(tf.float32, shape=None, name=\"lam\")\n",
    "\n",
    "    loss, W, b, accer = convolutional_layers(X, Y)\n",
    "\n",
    "    opt = tf.train.AdamOptimizer(0.0001).minimize(loss)\n",
    "\n",
    "    return W, b, Y, X, loss, opt, accer, lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Batch(itr,x_i,y_i,batch_size,reg,lam,sess,op,loss,X,Y,acc_o,Type):\n",
    "    d_s = (batch_size,x_i.shape[1]*x_i.shape[2])\n",
    "    t_s = (batch_size,1)\n",
    "    print(\"I am here\")\n",
    "    for j in range(itr):\n",
    "        print(\"Reaching\"+str(j))\n",
    "        x_batch = x_i[j*batch_size:(j+1)*batch_size].reshape(d_s)\n",
    "        y_batch = y_i[j*batch_size:(j+1)*batch_size]\n",
    "        if Type == 'train':\n",
    "            _, l, acc = sess.run([op,loss,acc_o],feed_dict={X:x_batch,Y:y_batch,lam:reg})\n",
    "            print(\"End\")\n",
    "        else:\n",
    "            l, acc = sess.run([loss,acc_o],feed_dict={X:x_batch,Y:y_batch,lam:reg})\n",
    "        print(\"End here\")\n",
    "        print(l,acc)\n",
    "    return l, acc\n",
    "def SGD(batch_size,iterations):\n",
    "    trainData, validData, testData, trainTarget, validTarget,testTarget = loadData()\n",
    "    trainTarget, validTarget, testTarget = convertOneHot(trainTarget, validTarget, testTarget)\n",
    "    W, b, Y, X, loss, op, acc_o, lam = Model_Training(trainData,trainTarget)\n",
    "    l_tr = []\n",
    "    l_v = []\n",
    "    l_t = []\n",
    "    a_tr = []\n",
    "    a_v = []\n",
    "    a_t = []\n",
    "    tr_batch = int(trainData.shape[0]/batch_size)\n",
    "    v_batch = int(validData.shape[0]/batch_size)\n",
    "    t_batch = int(testData.shape[0]/batch_size)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.local_variables_initializer())\n",
    "        for i in range(iterations):\n",
    "            tr_r = np.arange(trainData.shape[0])\n",
    "            np.random.shuffle(tr_r)\n",
    "            trainData = trainData[tr_r]\n",
    "            trainTarget = trainTarget[tr_r]\n",
    "            v_r = np.arange(validData.shape[0])\n",
    "            np.random.shuffle(v_r)\n",
    "            validData = validData[v_r]\n",
    "            validTarget = validTarget[v_r]\n",
    "            t_r = np.arange(testData.shape[0])\n",
    "            np.random.shuffle(t_r)\n",
    "            testData = testData[t_r]\n",
    "            testTarget = testTarget[t_r]\n",
    "            #Batch(itr,x_i,y_i,batch_size,reg,lam,sess,op,loss,X,Y,acc,type)\n",
    "            loss, acc = Batch(tr_batch,trainData,trainTarget,batch_size,reg,lam,sess,op,loss,X,Y,acc_o,'train')\n",
    "            e_v, acc_v = Batch(v_batch,validData,validTarget,batch_size,reg,lam,sess,op,loss,X,Y,acc_o,'valid')\n",
    "            e_t, acc_t = Batch(t_batch,testData,testTarget,batch_size,reg,lam,sess,op,loss,X,Y,acc_o,'test')\n",
    "            l_tr.append(loss)\n",
    "            l_v.append(e_v)\n",
    "            l_t.append(e_t)\n",
    "            a_tr.append(acc)\n",
    "            a_v.append(acc_v)\n",
    "            a_t.append(acc_t)\n",
    "    return l_tr, l_v, l_t, a_tr, a_v, a_t\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am here\n",
      "Reaching0\n",
      "End\n",
      "End here\n",
      "2.6278086 0.09375\n",
      "Reaching1\n",
      "End\n",
      "End here\n",
      "2.533319 0.15625\n",
      "Reaching2\n",
      "End\n",
      "End here\n",
      "1.5706352 0.28125\n",
      "Reaching3\n",
      "End\n",
      "End here\n",
      "1.4417468 0.3203125\n",
      "Reaching4\n",
      "End\n",
      "End here\n",
      "1.2288163 0.36875\n",
      "Reaching5\n"
     ]
    }
   ],
   "source": [
    "trainData, validData, testData, trainTarget, validTarget, testTarget = loadData()\n",
    "trainTarget, validTarget, testTarget = convertOneHot(trainTarget, validTarget, testTarget)\n",
    "l_train, l_valid, l_test, a_train, a_valid, a_test = SGD(32, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
