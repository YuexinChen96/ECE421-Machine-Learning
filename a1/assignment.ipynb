{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.set_printoptions(threshold = np.nan)\n",
    "def loadData():\n",
    "    with np.load('notMNIST.npz') as data :\n",
    "        Data, Target = data ['images'], data['labels']\n",
    "        #print(Data[3745])\n",
    "        #plt.figure()\n",
    "        #plt.imshow(Data[3745])\n",
    "        #plt.show()\n",
    "        #print(Target)     # 0-9\n",
    "        posClass = 2\n",
    "        negClass = 9\n",
    "        dataIndx = (Target==posClass) + (Target==negClass)\n",
    "        #true =1 false=-1?\n",
    "        #print(dataIndx)   #true or false\n",
    "        #print(Data[dataIndx])  #Data[true]\n",
    "        Data = Data[dataIndx]/255.\n",
    "        #print(Data[0])\n",
    "        Target = Target[dataIndx].reshape(-1, 1)\n",
    "        #print(Target[10])\n",
    "        #Target [size, 1]\n",
    "        Target[Target==posClass] = 1\n",
    "        Target[Target==negClass] = 0\n",
    "        np.random.seed(421)\n",
    "        randIndx = np.arange(len(Data))\n",
    "        #print(randIndx)\n",
    "        np.random.shuffle(randIndx)\n",
    "        Data, Target = Data[randIndx], Target[randIndx]\n",
    "        #print(Target)\n",
    "        trainData, trainTarget = Data[:3500], Target[:3500]\n",
    "        validData, validTarget = Data[3500:3600], Target[3500:3600]\n",
    "        testData, testTarget = Data[3600:], Target[3600:]\n",
    "    return trainData, validData, testData, trainTarget, validTarget, testTarget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(W, b, x, y, reg):\n",
    "    loss = 0\n",
    "    for i in range(0,len(y)):\n",
    "        traning_data = x[i].flatten()\n",
    "        loss =1/(2*len(y))*(np.dot(np.transpose(W),traning_data) + b - y[i])**2 + loss\n",
    "    loss = loss + reg/2 * np.dot(np.transpose(W), W)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradMSE(W, b, x, y, reg):\n",
    "    grad_W = 0\n",
    "    grad_b = 0\n",
    "    for i in range(0,len(y)):\n",
    "        traning_data = x[i].flatten()\n",
    "        grad_W = (1/len(y)) * (np.dot(np.transpose(W),traning_data) + b - y[i]) * traning_data + grad_W\n",
    "        grad_b = (1/len(y)) * (np.dot(np.transpose(W),traning_data) + b - y[i]) + grad_b\n",
    "    grad_W = grad_W + reg * W\n",
    "    return grad_W, grad_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_grad_descent(W, b, x, y, alpha, epochs, reg, error_tol):\n",
    "    old_loss = 0;\n",
    "    rate_losses = []\n",
    "    validate_losses = []\n",
    "    test_losses = []\n",
    "    for i in range(0,epochs):\n",
    "        \n",
    "        new_loss = MSE(W,b,x,y,reg)\n",
    "        validate_loss = MSE(W,b,validData,validTarget,reg)\n",
    "        test_loss = MSE(W,b,testData,testTarget,reg)\n",
    "        \n",
    "        grad_W, grad_b = gradMSE(W,b,x,y,reg)\n",
    "        W = W - grad_W * alpha\n",
    "        b = b - grad_b * alpha\n",
    "        if abs(new_loss - old_loss) < error_tol:\n",
    "            final_W = W\n",
    "            final_b = b\n",
    "        old_loss = new_loss\n",
    "        #print(new_loss, i)\n",
    "        rate_losses.append(new_loss)\n",
    "        validate_losses.append(validate_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        \n",
    "    return rate_losses,validate_losses,test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData, validData, testData, trainTarget, validTarget, testTarget = loadData()\n",
    "weight = np.zeros(len(trainData[0].flatten()))\n",
    "rate_losses,validate_losses,test_losses  = batch_grad_descent(weight,0, trainData, trainTarget, 0.005, 5000,0.5,1*10e-7)\n",
    "print(rate_losses,validate_losses,test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0)\n",
    "plt.title(\"regularization parameter is 0.5\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.plot(range(len(rate_losses)), rate_losses, 'g--', label=\"training\")\n",
    "plt.plot(range(len(validate_losses)), validate_losses, label=\"validate\")\n",
    "plt.plot(range(len(test_losses)), test_losses, label=\"testing\")\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_grad_descent_2(W, b, x, y, alpha, epochs, reg, error_tol):\n",
    "    old_loss = 0;\n",
    "    rate_losses = []\n",
    "    for i in range(0,epochs):\n",
    "        \n",
    "        new_loss = MSE(W,b,x,y,reg)      \n",
    "        grad_W, grad_b = gradMSE(W,b,x,y,reg)\n",
    "        W = W - grad_W * alpha\n",
    "        b = b - grad_b * alpha\n",
    "        if abs(new_loss - old_loss) < error_tol:\n",
    "            final_W = W\n",
    "            final_b = b\n",
    "        old_loss = new_loss\n",
    "        #print(new_loss,validate_loss,test_loss, i)\n",
    "        rate_losses.append(new_loss)\n",
    "    return rate_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData, validData, testData, trainTarget, validTarget, testTarget = loadData()\n",
    "weight = np.zeros(len(trainData[0].flatten()))\n",
    "first_losses  = batch_grad_descent_2(weight,0, trainData, trainTarget, 0.005, 5000,0.001,1e-7)\n",
    "second_losses  = batch_grad_descent_2(weight,0, trainData, trainTarget, 0.005, 5000,0.1,1e-7)\n",
    "third_losses  = batch_grad_descent_2(weight,0, trainData, trainTarget, 0.005, 5000,0.5,1e-7)\n",
    "print(first_losses,second_losses,third_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0)\n",
    "plt.title(\"regularization parameter vs loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.plot(range(len(first_losses)), first_losses, 'g--', label=\"0.001\")\n",
    "plt.plot(range(len(second_losses)), second_losses, label=\"0.1\")\n",
    "plt.plot(range(len(third_losses)), third_losses, label=\"0.5\")\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(first_losses[-1], second_losses[-1], third_losses[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_equation(x,y):\n",
    "    training = np.empty((len(y),len(trainData[0].flatten())))\n",
    "    for i in range(0,len(y)):\n",
    "        training[i] = x[i].flatten()\n",
    "    final = np.dot(np.linalg.inv(np.dot(np.transpose(training),training)),np.dot(np.transpose(training),y))\n",
    "    return final_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData, validData, testData, trainTarget, validTarget, testTarget = loadData()\n",
    "final_weight = normal_equation(trainData,trainTarget)\n",
    "result = MSE(final_weight, 0, trainData, trainTarget, 0)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossEntropyLoss(W, b, x, y, reg):\n",
    "    loss = 0\n",
    "    for i in range(0,len(y)):\n",
    "        traning_data = x[i].flatten()\n",
    "        y_bar = np.dot(np.transpose(W),traning_data) + b\n",
    "        loss =1/(len(y))*(np.log(1 + np.exp(-y_bar)) + (1-y[i])*y_bar) + loss\n",
    "    loss = loss + reg/2 * np.dot(np.transpose(W), W)\n",
    "    #print(y_bar)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradCE(W, b, x, y, reg):\n",
    "    grad_W = 0\n",
    "    grad_b = 0.0\n",
    "    for i in range(0,len(y)):\n",
    "        traning_data = x[i].flatten()\n",
    "        expo_fun = np.exp(-(np.dot(np.transpose(W),traning_data) + b))\n",
    "        grad_W = (1/len(y)) * ((1-y[i]) - (expo_fun/(1+expo_fun))) * traning_data + grad_W\n",
    "        grad_b = (1/len(y)) * (1/(1+expo_fun) - y[i]) + grad_b\n",
    "    grad_W = grad_W + reg * W\n",
    "    #print(grad_W)\n",
    "    return grad_W, grad_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_descent(W, b, x, y, alpha, iterations, reg, EPS, lossType=\"None\"):\n",
    "    old_loss = 0;\n",
    "    rate_losses = []\n",
    "    validate_losses = []\n",
    "    test_losses = []\n",
    "    for i in range(0,iterations):\n",
    "        if(lossType == \"b\"):\n",
    "            new_loss = MSE(W,b,x,y,reg)\n",
    "            #validate_loss = MSE(W,b,validData,validTarget,reg)\n",
    "            #test_loss = MSE(W,b,testData,testTarget,reg)\n",
    "\n",
    "            grad_W, grad_b = gradMSE(W,b,x,y,reg)\n",
    "            W = W - grad_W * alpha\n",
    "            b = b - grad_b * alpha\n",
    "            if abs(new_loss - old_loss) < EPS:\n",
    "                final_W = W\n",
    "                final_b = b\n",
    "            old_loss = new_loss\n",
    "            rate_losses.append(new_loss)\n",
    "            #validate_losses.append(validate_loss)\n",
    "            #test_losses.append(test_loss)\n",
    "            #print(new_loss, i)\n",
    "        elif(lossType == \"c\"):\n",
    "            new_loss = crossEntropyLoss(W,b,x,y,reg)\n",
    "            #validate_loss = crossEntropyLoss(W,b,validData,validTarget,reg)\n",
    "            #test_loss = crossEntropyLoss(W,b,testData,testTarget,reg)\n",
    "            grad_W, grad_b = gradCE(W,b,x,y,reg)\n",
    "            W = W - grad_W * alpha\n",
    "            b = b - grad_b * alpha\n",
    "            if abs(new_loss - old_loss) < EPS:\n",
    "                final_W = W\n",
    "                final_b = b\n",
    "            old_loss = new_loss\n",
    "            rate_losses.append(new_loss)\n",
    "            #validate_losses.append(validate_loss)\n",
    "            #test_losses.append(test_loss)\n",
    "            \n",
    "            #print(new_loss, i)\n",
    "    return rate_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildGraph(beta1=None, beta2=None, epsilon=None, lossType=None, learning_rate=None):\n",
    "    # Your implementation here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData, validData, testData, trainTarget, validTarget, testTarget = loadData()\n",
    "weight = np.zeros(len(trainData[0].flatten()))\n",
    "first_losses, second_losses, third_losses = grad_descent(weight, 0, trainData,trainTarget, 0.005, 5000, 0.1, 1*10e-7, \"c\")\n",
    "print(first_losses, second_losses, third_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(first_losses[-1], second_losses[-1], third_losses[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0)\n",
    "plt.title(\"Logistic regression (reg 0.005)\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.plot(range(700), first_losses[:700], 'g--', label=\"training\")\n",
    "plt.plot(range(700), second_losses[:700], 'r--', label=\"validate\")\n",
    "plt.plot(range(700), third_losses[:700], 'y--',label=\"testing\")\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_CE(W, b, x, y, reg):\n",
    "    correct = 0\n",
    "    for i in range(0,len(y)):\n",
    "        traning_data = x[i].flatten()\n",
    "        sigma = 1/(1+ np.exp(-(np.dot(np.transpose(W),traning_data) + b)))\n",
    "        if(y[i] == 0 and sigma <=0.5):\n",
    "            correct = correct +1\n",
    "        elif(y[i] == 1 and sigma >0.5):\n",
    "            correct = correct +1\n",
    "    accuracy_rate = (correct/len(y))*100\n",
    "    #print(accuracy_rate)\n",
    "    return accuracy_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(W, b, x, y, alpha, iterations, reg, EPS, lossType=\"None\"):\n",
    "    old_loss = 0;\n",
    "    rate_losses = []\n",
    "    validate_losses = []\n",
    "    test_losses = []\n",
    "    for i in range(0,iterations):\n",
    "        if(lossType == \"b\"):\n",
    "            new_loss = MSE(W,b,x,y,reg)\n",
    "            validate_loss = MSE(W,b,validData,validTarget,reg)\n",
    "            test_loss = MSE(W,b,testData,testTarget,reg)\n",
    "            grad_W, grad_b = gradMSE(W,b,x,y,reg)\n",
    "\n",
    "            W = W - grad_W * alpha\n",
    "            b = b - grad_b * alpha\n",
    "            if abs(new_loss - old_loss) < EPS:\n",
    "                final_W = W\n",
    "                final_b = b\n",
    "            old_loss = new_loss\n",
    "            rate_losses.append(new_loss)\n",
    "            #validate_losses.append(validate_loss)\n",
    "            #test_losses.append(test_loss)\n",
    "        elif(lossType == \"c\"):\n",
    "            new_loss = accuracy_CE(W,b,x,y,reg)\n",
    "            validate_loss = accuracy_CE(W,b,validData,validTarget,reg)\n",
    "            test_loss = accuracy_CE(W,b,testData,testTarget,reg)\n",
    "            grad_W, grad_b = gradCE(W,b,x,y,reg)\n",
    "            W = W - grad_W * alpha\n",
    "            b = b - grad_b * alpha\n",
    "            if abs(new_loss - old_loss) < EPS:\n",
    "                final_W = W\n",
    "                final_b = b\n",
    "            old_loss = new_loss\n",
    "            rate_losses.append(new_loss)\n",
    "            validate_losses.append(validate_loss)\n",
    "            test_losses.append(test_loss)\n",
    "            \n",
    "            #print(new_loss, i)\n",
    "    return rate_losses,validate_losses,test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData, validData, testData, trainTarget, validTarget, testTarget = loadData()\n",
    "weight = np.zeros(len(trainData[0].flatten()))\n",
    "first_losses, second_losses, third_losses = accuracy(weight, 0, trainData,trainTarget,  0.005, 5000, 0.1, 1*10e-7, \"c\")\n",
    "print(first_losses, second_losses, third_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0)\n",
    "plt.title(\"Accuracy curve (reg 0.005)\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.plot(range(700), first_losses[1:701], 'g--', label=\"training\")\n",
    "plt.plot(range(700), second_losses[1:701], 'r--', label=\"validate\")\n",
    "plt.plot(range(700), third_losses[1:701], 'y--',label=\"testing\")\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(first_losses), max(second_losses), max(third_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData, validData, testData, trainTarget, validTarget, testTarget = loadData()\n",
    "weight = np.zeros(len(trainData[0].flatten()))\n",
    "#first_losses = grad_descent(weight, 0, trainData,trainTarget, 0.005, 5000, 0.1, 1*10e-7, \"b\")\n",
    "second_losses = grad_descent(weight, 0, trainData,trainTarget, 0.005, 5000, 0.1, 1*10e-7, \"c\")\n",
    "print(first_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0xb3a00c4a8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(0)\n",
    "plt.title(\"linear vs logistics (reg 0.005)\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.plot(range(700), first_losses[0:700], 'g--', label=\"linear\")\n",
    "plt.plot(range(700), second_losses[0:700],'r--', label=\"logistic\")\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(first_losses.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_MSE(W, b, x, y, reg):\n",
    "    correct = 0\n",
    "    for i in range(0,len(y)):\n",
    "        traning_data = x[i].flatten()\n",
    "        y_hat = np.dot(np.transpose(W),traning_data) + b\n",
    "        if(y[i] == 0 and np.sign(y_hat)== -1):\n",
    "            correct = correct +1\n",
    "        elif(y[i] == 1 and np.sign(y_hat)== 1):\n",
    "            correct = correct +1\n",
    "    accuracy_rate = (correct/len(y))*100\n",
    "    return accuracy_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
