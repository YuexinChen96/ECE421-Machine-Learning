{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.set_printoptions(threshold = np.nan)\n",
    "def loadData():\n",
    "    with np.load('notMNIST.npz') as data :\n",
    "        Data, Target = data ['images'], data['labels']\n",
    "        #print(Data[3745])\n",
    "        #plt.figure()\n",
    "        #plt.imshow(Data[3745])\n",
    "        #plt.show()\n",
    "        #print(Target)     # 0-9\n",
    "        posClass = 2\n",
    "        negClass = 9\n",
    "        dataIndx = (Target==posClass) + (Target==negClass)\n",
    "        #true =1 false=-1?\n",
    "        #print(dataIndx)   #true or false\n",
    "        #print(Data[dataIndx])  #Data[true]\n",
    "        Data = Data[dataIndx]/255.\n",
    "        #print(Data[0])\n",
    "        Target = Target[dataIndx].reshape(-1, 1)\n",
    "        #print(Target[10])\n",
    "        #Target [size, 1]\n",
    "        Target[Target==posClass] = 1\n",
    "        Target[Target==negClass] = 0\n",
    "        np.random.seed(421)\n",
    "        randIndx = np.arange(len(Data))\n",
    "        #print(randIndx)\n",
    "        np.random.shuffle(randIndx)\n",
    "        Data, Target = Data[randIndx], Target[randIndx]\n",
    "        #print(Target)\n",
    "        trainData, trainTarget = Data[:3500], Target[:3500]\n",
    "        validData, validTarget = Data[3500:3600], Target[3500:3600]\n",
    "        testData, testTarget = Data[3600:], Target[3600:]\n",
    "    return trainData, validData, testData, trainTarget, validTarget, testTarget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(W, b, x, y, reg):\n",
    "    loss = 0\n",
    "    for i in range(0,len(y)):\n",
    "        traning_data = x[i].flatten()\n",
    "        loss =1/(2*len(y))*(np.dot(np.transpose(W),traning_data) + b - y[i])**2 + loss\n",
    "    loss = loss + reg/2 * np.dot(np.transpose(W), W)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradMSE(W, b, x, y, reg):\n",
    "    grad_W = 0\n",
    "    grad_b = 0\n",
    "    for i in range(0,len(y)):\n",
    "        traning_data = x[i].flatten()\n",
    "        grad_W = (1/len(y)) * (np.dot(np.transpose(W),traning_data) + b - y[i]) * traning_data + grad_W\n",
    "        grad_b = (1/len(y)) * (np.dot(np.transpose(W),traning_data) + b - y[i]) + grad_b\n",
    "    grad_W = grad_W + reg * W\n",
    "    return grad_W, grad_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_grad_descent(W, b, x, y, alpha, epochs, reg, error_tol):\n",
    "    old_loss = 0;\n",
    "    rate_losses = []\n",
    "    validate_losses = []\n",
    "    test_losses = []\n",
    "    for i in range(0,epochs):\n",
    "        \n",
    "        new_loss = MSE(W,b,x,y,reg)\n",
    "        validate_loss = MSE(W,b,validData,validTarget,reg)\n",
    "        test_loss = MSE(W,b,testData,testTarget,reg)\n",
    "        \n",
    "        grad_W, grad_b = gradMSE(W,b,x,y,reg)\n",
    "        W = W - grad_W * alpha\n",
    "        b = b - grad_b * alpha\n",
    "        if abs(new_loss - old_loss) < error_tol:\n",
    "            final_W = W\n",
    "            final_b = b\n",
    "        old_loss = new_loss\n",
    "        #print(new_loss,validate_loss,test_loss, i)\n",
    "        rate_losses.append(new_loss)\n",
    "        validate_losses.append(validate_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        \n",
    "    return rate_losses,validate_losses,test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph():\n",
    "    #rates = [0.005, 0.001, 0.0001]\n",
    "    rate_losses = []\n",
    "    plt.figure(0)\n",
    "    plt.title(\"Q1\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    for rate in rates:\n",
    "        rate_losses = batch_grad_descent(W, b, x, y, alpha, epochs, reg, error_tol)\n",
    "        plt.plot(range(len(rate_losses)), losses, label=\"rate=\" + str(rate))\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    return rates[rate_losses.index(min(rate_losses))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData, validData, testData, trainTarget, validTarget, testTarget = loadData()\n",
    "weight = np.zeros(len(trainData[0].flatten()))\n",
    "rate_losses,validate_losses,test_losses  = batch_grad_descent(weight,0, trainData, trainTarget, 0.005, 5000,0.5,1e-7)\n",
    "print(rate_losses,validate_losses,test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0)\n",
    "plt.title(\"regularization parameter is 0.5\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.plot(range(len(rate_losses)), rate_losses, 'g--', label=\"training\")\n",
    "plt.plot(range(len(validate_losses)), validate_losses, label=\"validate\")\n",
    "plt.plot(range(len(test_losses)), test_losses, label=\"testing\")\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_grad_descent_2(W, b, x, y, alpha, epochs, reg, error_tol):\n",
    "    old_loss = 0;\n",
    "    rate_losses = []\n",
    "    for i in range(0,epochs):\n",
    "        \n",
    "        new_loss = MSE(W,b,x,y,reg)      \n",
    "        grad_W, grad_b = gradMSE(W,b,x,y,reg)\n",
    "        W = W - grad_W * alpha\n",
    "        b = b - grad_b * alpha\n",
    "        if abs(new_loss - old_loss) < error_tol:\n",
    "            final_W = W\n",
    "            final_b = b\n",
    "        old_loss = new_loss\n",
    "        #print(new_loss,validate_loss,test_loss, i)\n",
    "        rate_losses.append(new_loss)\n",
    "    return rate_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData, validData, testData, trainTarget, validTarget, testTarget = loadData()\n",
    "weight = np.zeros(len(trainData[0].flatten()))\n",
    "first_losses  = batch_grad_descent_2(weight,0, trainData, trainTarget, 0.005, 5000,0.001,1e-7)\n",
    "second_losses  = batch_grad_descent_2(weight,0, trainData, trainTarget, 0.005, 5000,0.1,1e-7)\n",
    "third_losses  = batch_grad_descent_2(weight,0, trainData, trainTarget, 0.005, 5000,0.5,1e-7)\n",
    "print(first_losses,second_losses,third_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0)\n",
    "plt.title(\"regularization parameter vs loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.plot(range(len(first_losses)), first_losses, 'g--', label=\"0.001\")\n",
    "plt.plot(range(len(second_losses)), second_losses, label=\"0.1\")\n",
    "plt.plot(range(len(third_losses)), third_losses, label=\"0.5\")\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01372196] [0.01672046] [0.02039289]\n"
     ]
    }
   ],
   "source": [
    "print(first_losses[-1], second_losses[-1], third_losses[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_equation(x,y):\n",
    "    training = np.empty((len(y),len(trainData[0].flatten())))\n",
    "    for i in range(0,len(y)):\n",
    "        training[i] = x[i].flatten()\n",
    "    final = np.dot(np.linalg.inv(np.dot(np.transpose(training),training)),np.dot(np.transpose(training),y))\n",
    "    return final_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01158202]]\n"
     ]
    }
   ],
   "source": [
    "trainData, validData, testData, trainTarget, validTarget, testTarget = loadData()\n",
    "final_weight = normal_equation(trainData,trainTarget)\n",
    "result = MSE(final_weight, 0, trainData, trainTarget, 0)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossEntropyLoss(W, b, x, y, reg):\n",
    "    loss = 0\n",
    "    for i in range(0,len(y)):\n",
    "        traning_data = x[i].flatten()\n",
    "        y_bar = 1/(1+exp(-(np.dot(np.transpose(W),traning_data + b))))\n",
    "        loss =1/(len(y))*(-y[i]*np.log(y_bar) - (1-y[i])*np.log(1-y_bar)) + loss\n",
    "    loss = loss + reg/2 * np.dot(np.transpose(W), W)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradCE(W, b, x, y, reg):\n",
    "    # Your implementation here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_descent(W, b, trainingData, trainingLabels, alpha, iterations, reg, EPS):\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildGraph(beta1=None, beta2=None, epsilon=None, lossType=None, learning_rate=None):\n",
    "    trainData, validData, testData, trainTarget, validTarget, testTarget = loadData()\n",
    "    dim_x = trainData.shape[1]\n",
    "    dim_y = trainData.shape[2]\n",
    "\n",
    "    tf.set_random_seed(421)\n",
    "    W_shape = (dim_x*dim_y, 1)\n",
    "    W = tf.get_variable(\"W\", initializer=tf.truncated_normal(shape=W_shape, stddev=0.5))\n",
    "    b = tf.get_variable(\"b\", initializer=tf.truncated_normal(shape=[1], stddev=0.5))\n",
    "\n",
    "    X = tf.placeholder(tf.float32, shape=(1750, dim_x*dim_y), name=\"X\")\n",
    "    Y = tf.placeholder(tf.float32, shape=(1750, 1), name=\"Y\")\n",
    "    lam = tf.placeholder(tf.float32, shape=(1, None), name=\"lam\")\n",
    "\n",
    "    predict = None\n",
    "    loss = None\n",
    "    if lossType == \"MSE\":\n",
    "        predict = tf.matmul(X, W) + b\n",
    "        loss = tf.losses.mean_squared_error(labels=Y, predictions=predict)\n",
    "    elif lossType == \"CE\":\n",
    "        logit = -1*(tf.matmul(X, W) + b)\n",
    "        predict = tf.sigmoid(logit)\n",
    "        loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logit)\n",
    "\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=beta1, beta2=beta2, epsilon=epsilon).minimize(loss)\n",
    "\n",
    "    return W, b, predict, Y, X, loss, train_op, lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BuildGraph(beta1=None, beta2=None, epsilon=None, lossType=None, learning_rate=None):\n",
    "    \n",
    "    # Your implementation here\n",
    "    tf.set_random_seed(421)\n",
    "    W = tf.truncated_normal(shape = (28*28, 1), stddev = 0.5, dtype = tf.float32, seed = 421, name = \"Weight\")\n",
    "    b = tf.truncated_normal(shape = (1,1),stddev = 0.5, dtype = tf.float32, seed = 421, name = \"Bias\")\n",
    "    X = tf.placeholder(tf.float32)\n",
    "    Y = tf.placeholder(tf.float32)\n",
    "    \n",
    "    estimate = tf.matmul(X,W) + b\n",
    "    predict = None\n",
    "    error = None\n",
    "    if lossType == \"MSE\":\n",
    "        predict = estimate\n",
    "        loss = tf.losses.mean_squared_error(labels=Y, predictions=predict)\n",
    "    elif lossType == \"CE\":\n",
    "        logit = -1*estimate\n",
    "        predict = tf.sigmoid(logit)\n",
    "        loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=Y,logits=logit)\n",
    "    print(loss)\n",
    "    train = tf.train.AdamOptimizer(learning_rate=0.001,beta1=beta1,beta2=beta2,epsilon=epsilon).minimize(loss)\n",
    "    return W,b,X,predict,Y,loss,train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildGraph2(beta1=None, beta2=None, epsilon=None, lossType=None, learning_rate=None):\n",
    "    #Initialize weight and bias tensors\n",
    "    tf.set_random_seed(421)\n",
    "    W = tf.truncated_normal(shape = (28*28, 1), stddev = 0.5, dtype = tf.float32, seed = 421, name = \"weight\")\n",
    "    B = tf.truncated_normal(shape = (1,1), stddev = 0.5, dtype = tf.float32, seed = 421, name = \"bias\")\n",
    "    x = tf.placeholder(tf.float32, shape = (3200, 28*28))\n",
    "    y = tf.placeholder(tf.float32, shape = (3200, 1))\n",
    "    if lossType == \"MSE\":   \n",
    "        return SGD2(x, y, W, B, learning_rate, reg, epoch, batch_size)\n",
    "    elif lossType == \"CE\":\n",
    "        return SGD2(x, y, W, B, learning_rate, reg, epoch, batch_size, opt = \"Adam\", beta1_ = b1, beta2_ = b2, epsilon_ = epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(batchSize, epochs):\n",
    "    trainData, validData, testData, trainTarget, validTarget, testTarget = loadData()\n",
    "\n",
    "    batches = trainData.shape[0]/batchSize\n",
    "    W, b, predict, Y, X, loss, train_op, lam = buildGraph(beta1=1, beta2=1, epsilon=0, lossType=\"CE\", learning_rate=0.001)\n",
    "    losses = []\n",
    "\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "\n",
    "        for i in range(epochs):\n",
    "            rand_i = np.random.choice(100, size=batchSize)\n",
    "\n",
    "            x_batch = trainData[rand_i].reshape((batchSize, trainData.shape[1]*trainData.shape[2]))\n",
    "            y_batch = trainTarget[rand_i].reshape((batchSize, 1))\n",
    "            _, c = sess.run([train_op, loss], feed_dict={X:x_batch, Y:y_batch})\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tensor(epochs,trainData,trainTarget,typeError,batch_size):\n",
    "    W,b,X,predict,Y,error,train = buildGraph(lossType=typeError)\n",
    "    init = tf.global_variables_initializer()\n",
    "    traning_data = x[i].flatten()\n",
    "    sess = tf.InteractiveSession()\n",
    "    sess.run(init)\n",
    "    \n",
    "    total_E = []\n",
    "    total_A = []\n",
    "    i = 0\n",
    "    for a in range(0,epochs):\n",
    "        ins = np.shape(trainData)[0]\n",
    "        t_batches = int(ins/batch_size)\n",
    "        idx = np.random.permutation(len(trainData))\n",
    "        X_r, Y_r = traning_data[idx], trainTarget[idx]\n",
    "        \n",
    "        i = 0\n",
    "        \n",
    "        for b in range(t_batches):\n",
    "            X_batch = X_r[i:(i+batch_size),:]\n",
    "            Y_batch = Y_r[i:(i+batch_size),:]\n",
    "            _,err,currentW,currentb,yh = sess.run([train,error,W,b,predict])\n",
    "            i = i + batch\n",
    "    for c in range(len(yh)):\n",
    "        print(yh)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SGD(1750,700)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
